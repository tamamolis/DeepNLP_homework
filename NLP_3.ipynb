{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randrange\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "у меня jupyter notebook почему-то не видел этот файлик, хотя они были в одной папке, поэтому я \n",
    "вставила функции просто в блок, по-другому не получалось\n",
    "\"\"\"\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "  # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "              print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "    \n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n",
    "    numeric_diffs = []\n",
    "    for input_blob in inputs:\n",
    "        diff = np.zeros_like(input_blob.diffs)\n",
    "        it = np.nditer(input_blob.vals, flags=['multi_index'],\n",
    "                   op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            orig = input_blob.vals[idx]\n",
    "            input_blob.vals[idx] = orig + h\n",
    "            f(*(inputs + (output,)))\n",
    "            pos = np.copy(output.vals)\n",
    "            input_blob.vals[idx] = orig - h\n",
    "            f(*(inputs + (output,)))\n",
    "            neg = np.copy(output.vals)\n",
    "            input_blob.vals[idx] = orig\n",
    "      \n",
    "            diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n",
    "\n",
    "            it.iternext()\n",
    "        numeric_diffs.append(diff)\n",
    "        \n",
    "    return numeric_diffs\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n",
    "    return eval_numerical_gradient_blobs(lambda *args: net.forward(),\n",
    "              inputs, output, h=h)\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "    for i in xrange(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evaluate f(x + h)\n",
    "        x[ix] = oldval - h # increment by h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + \n",
    "                                                           abs(grad_analytic))\n",
    "        print ('numerical: %f analytic: %f, relative error: %e' % \n",
    "               (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16\n",
    "\n",
    "https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/\n",
    "\n",
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_solver.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \n",
    "    probs = np.exp(x - np.max (x, axis=1, keepdims=True))\n",
    "    probs /= np.sum (probs, axis=1, keepdims=True)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    \n",
    "    loss = -np.sum(np.log(probs[np.arange(N), y])) / N \n",
    "    dx = probs.copy()    \n",
    "    dx[np.arange(N), y] -= 1    \n",
    "    dx /= N\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      " 1.02024765831\n",
      "gradient is a matrix with shape 10x3\n",
      " [-0.07311599  0.02776     0.045356    0.02083207  0.04395216 -0.06478424\n",
      " -0.07878944  0.04836753  0.03042191  0.03209705 -0.07874838  0.04665133\n",
      " -0.06562582  0.04061855  0.02500726  0.03531997  0.03547021 -0.07079018\n",
      " -0.05161597  0.02558227  0.0260337  -0.07136769  0.03489463  0.03647307\n",
      "  0.03562948  0.0367558  -0.07238527  0.03043723 -0.06206345  0.03162621]\n",
      "difference should be ~10e-8 4.53771950311e-08\n"
     ]
    }
   ],
   "source": [
    "print('loss is a scalar\\n', loss(np.random.random((10, 3))))\n",
    "print('gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3))))\n",
    "print('difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mlxai.github.io/2017/01/10/a-modular-approach-to-implementing-fully-connected-neural-networks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    out = None\n",
    "        \n",
    "    pass\n",
    "\n",
    "    # out = x*W + b\n",
    "    out = x.reshape(x.shape[0], w.shape[0]).dot(w) + b\n",
    "    cache = (x, w, b)\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print ('Testing affine_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    \n",
    "    pass\n",
    "    \n",
    "    dx = dout.dot(w.T).reshape(x.shape)\n",
    "    dw = x.reshape(x.shape[0], w.shape[0]).T.dot(dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  6.99765929149e-10\n",
      "dw error:  1.34027358451e-10\n",
      "db error:  1.31765828013e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print ('Testing affine_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))\n",
    "print ('dw error: ', rel_error(dw_num, dw))\n",
    "print ('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/relu_layer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \n",
    "    out = None\n",
    "    pass\n",
    "    relu = lambda x: x * (x > 0).astype(float)\n",
    "    out = relu(x)\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print ('Testing relu_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "\n",
    "    dx, x = None, cache\n",
    "    pass\n",
    "    \n",
    "    dx = dout * (x >= 0)\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27561945151e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print ('Testing relu_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x117868438>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNG4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176de4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/tamamo/neural-network-using-sgd/edit?unified=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "epoch 0: \t tr_loss inf \t te_loss inf \t te_acc 0.10555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000: \t tr_loss 6.29 \t te_loss 5.77 \t te_acc 0.6814814814814815\n",
      "epoch 2000: \t tr_loss 2.96 \t te_loss 2.56 \t te_acc 0.8092592592592592\n",
      "epoch 3000: \t tr_loss 0.80 \t te_loss 1.74 \t te_acc 0.8629629629629629\n",
      "epoch 4000: \t tr_loss 0.82 \t te_loss 1.38 \t te_acc 0.8777777777777778\n",
      "epoch 5000: \t tr_loss 0.61 \t te_loss 1.21 \t te_acc 0.8962962962962963\n",
      "epoch 6000: \t tr_loss 1.08 \t te_loss 1.16 \t te_acc 0.9\n",
      "epoch 7000: \t tr_loss 0.51 \t te_loss 1.00 \t te_acc 0.9037037037037037\n",
      "epoch 8000: \t tr_loss 0.13 \t te_loss 0.89 \t te_acc 0.9111111111111111\n",
      "epoch 9000: \t tr_loss 0.42 \t te_loss 0.78 \t te_acc 0.9314814814814815\n",
      "epoch 10000: \t tr_loss 0.39 \t te_loss 0.84 \t te_acc 0.924074074074074\n",
      "epoch 11000: \t tr_loss 0.73 \t te_loss 0.76 \t te_acc 0.9333333333333333\n",
      "epoch 12000: \t tr_loss 0.19 \t te_loss 0.74 \t te_acc 0.9314814814814815\n",
      "epoch 13000: \t tr_loss 0.29 \t te_loss 0.73 \t te_acc 0.9277777777777778\n",
      "epoch 14000: \t tr_loss 0.32 \t te_loss 0.83 \t te_acc 0.9277777777777778\n",
      "epoch 15000: \t tr_loss 0.04 \t te_loss 0.70 \t te_acc 0.9388888888888889\n",
      "epoch 16000: \t tr_loss 0.15 \t te_loss 0.66 \t te_acc 0.9388888888888889\n",
      "epoch 17000: \t tr_loss 0.55 \t te_loss 0.67 \t te_acc 0.9333333333333333\n",
      "epoch 18000: \t tr_loss 0.21 \t te_loss 0.65 \t te_acc 0.9425925925925925\n",
      "epoch 19000: \t tr_loss 0.20 \t te_loss 0.71 \t te_acc 0.9333333333333333\n",
      "epoch 20000: \t tr_loss 0.02 \t te_loss 0.68 \t te_acc 0.9407407407407408\n",
      "epoch 21000: \t tr_loss 0.07 \t te_loss 0.72 \t te_acc 0.9166666666666666\n",
      "epoch 22000: \t tr_loss 0.04 \t te_loss 0.64 \t te_acc 0.9425925925925925\n",
      "epoch 23000: \t tr_loss 0.02 \t te_loss 0.66 \t te_acc 0.9314814814814815\n",
      "epoch 24000: \t tr_loss 0.12 \t te_loss 0.86 \t te_acc 0.912962962962963\n",
      "epoch 25000: \t tr_loss 0.08 \t te_loss 0.64 \t te_acc 0.9425925925925925\n",
      "epoch 26000: \t tr_loss 0.00 \t te_loss 0.62 \t te_acc 0.937037037037037\n",
      "epoch 27000: \t tr_loss 0.12 \t te_loss 0.64 \t te_acc 0.9388888888888889\n",
      "epoch 28000: \t tr_loss 0.08 \t te_loss 0.60 \t te_acc 0.937037037037037\n",
      "epoch 29000: \t tr_loss 0.40 \t te_loss 0.69 \t te_acc 0.9259259259259259\n",
      "epoch 30000: \t tr_loss 0.00 \t te_loss 0.62 \t te_acc 0.937037037037037\n",
      "epoch 31000: \t tr_loss 0.02 \t te_loss 0.60 \t te_acc 0.937037037037037\n",
      "epoch 32000: \t tr_loss 0.05 \t te_loss 0.61 \t te_acc 0.9407407407407408\n",
      "epoch 33000: \t tr_loss 0.01 \t te_loss 0.57 \t te_acc 0.9407407407407408\n",
      "epoch 34000: \t tr_loss 0.02 \t te_loss 0.60 \t te_acc 0.9407407407407408\n",
      "epoch 35000: \t tr_loss 0.03 \t te_loss 0.62 \t te_acc 0.9333333333333333\n",
      "epoch 36000: \t tr_loss 0.01 \t te_loss 0.60 \t te_acc 0.9388888888888889\n",
      "epoch 37000: \t tr_loss 0.02 \t te_loss 0.59 \t te_acc 0.9425925925925925\n",
      "epoch 38000: \t tr_loss 0.03 \t te_loss 0.61 \t te_acc 0.937037037037037\n",
      "epoch 39000: \t tr_loss 0.04 \t te_loss 0.61 \t te_acc 0.9259259259259259\n",
      "epoch 40000: \t tr_loss 0.00 \t te_loss 0.56 \t te_acc 0.9425925925925925\n",
      "epoch 41000: \t tr_loss 0.02 \t te_loss 0.57 \t te_acc 0.9425925925925925\n",
      "epoch 42000: \t tr_loss 0.01 \t te_loss 0.57 \t te_acc 0.9388888888888889\n",
      "epoch 43000: \t tr_loss 0.02 \t te_loss 0.57 \t te_acc 0.9407407407407408\n",
      "epoch 44000: \t tr_loss 0.03 \t te_loss 0.58 \t te_acc 0.937037037037037\n",
      "epoch 45000: \t tr_loss 0.01 \t te_loss 0.56 \t te_acc 0.9388888888888889\n",
      "epoch 46000: \t tr_loss 0.01 \t te_loss 0.58 \t te_acc 0.9351851851851852\n",
      "epoch 47000: \t tr_loss 0.02 \t te_loss 0.53 \t te_acc 0.9388888888888889\n",
      "epoch 48000: \t tr_loss 0.02 \t te_loss 0.56 \t te_acc 0.937037037037037\n",
      "epoch 49000: \t tr_loss 0.00 \t te_loss 0.53 \t te_acc 0.937037037037037\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNG4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1174b6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')\n",
    "\n",
    "\n",
    "W1, b1 = np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    dx, dW2, db2 = affine_backward(dx, cache3)\n",
    "    dx = relu_backward(dx, cache2)\n",
    "    dx, dW1, db1 = affine_backward(dx, cache1)\n",
    "\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1    \n",
    "   \n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2,    W2, b2) # Dense Layer \n",
    "    te_loss, dx = softmax_loss(out3, y_test)         # Loss Layer \n",
    "\n",
    "    probs = np.exp(out3 - np.max(out3, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('epoch {}: \\t tr_loss {:.2f} \\t te_loss {:.2f} \\t te_acc {}'.\n",
    "              format(i, tr_loss , te_loss, accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
